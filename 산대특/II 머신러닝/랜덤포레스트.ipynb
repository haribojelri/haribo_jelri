{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec8c6c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.2-py3-none-win_amd64.whl (125.4 MB)\n",
      "     -------------------------------------- 125.4/125.4 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\miniconda3\\envs\\ai\\lib\\site-packages (from xgboost) (1.21.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\miniconda3\\envs\\ai\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c820c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting LightGBM\n",
      "  Downloading lightgbm-3.3.3-py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 9.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\user\\miniconda3\\envs\\ai\\lib\\site-packages (from LightGBM) (1.0.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\user\\miniconda3\\envs\\ai\\lib\\site-packages (from LightGBM) (0.37.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\miniconda3\\envs\\ai\\lib\\site-packages (from LightGBM) (1.21.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\miniconda3\\envs\\ai\\lib\\site-packages (from LightGBM) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\miniconda3\\envs\\ai\\lib\\site-packages (from scikit-learn!=0.22.0->LightGBM) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\miniconda3\\envs\\ai\\lib\\site-packages (from scikit-learn!=0.22.0->LightGBM) (3.1.0)\n",
      "Installing collected packages: LightGBM\n",
      "Successfully installed LightGBM-3.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "582e5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier ,ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "# 앙상블... 정형 데이터에서 가장 뛰어난 성능을 내는 머신러닝중 하나\n",
    "# 대표적\n",
    "# sklearn\n",
    "#  - 랜덤포레스트 : 부스트트랩 샘플->대표적인 앙상블\n",
    "#  - 엑스트라 트리 : 노드를 랜덤하게 분할\n",
    "#  - 그레디언트 부스팅 : 이전 트리의 손실을 보완하는 식으로 deepth가 얖은 결정트리를 연속해서 사용\n",
    "#  -  히스토그램 기반 그레디언트 부스팅 : 훈련데이터를 256개 정수구간으로 나누어서 빠르고 높은 성능\n",
    "# 그외  xgboost  LightGBM\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b15078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost  GBM보다 학습시간이 적고, 다양한 옵션 규제, 조기중단..\n",
    "# 대략 2년후... lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afca9190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LGBMClassifier in module lightgbm.sklearn:\n",
      "\n",
      "class LGBMClassifier(sklearn.base.ClassifierMixin, LGBMModel)\n",
      " |  LGBMClassifier(boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[str, Callable, NoneType] = None, class_weight: Union[Dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: int = -1, silent: Union[bool, str] = 'warn', importance_type: str = 'split', **kwargs)\n",
      " |  \n",
      " |  LightGBM classifier.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LGBMClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      LGBMModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, init_score=None, eval_set=None, eval_names=None, eval_sample_weight=None, eval_class_weight=None, eval_init_score=None, eval_metric=None, early_stopping_rounds=None, verbose='warn', feature_name='auto', categorical_feature='auto', callbacks=None, init_model=None)\n",
      " |      Build a gradient boosting model from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Input feature matrix.\n",
      " |      y : array-like of shape = [n_samples]\n",
      " |          The target values (class labels in classification, real numbers in regression).\n",
      " |      sample_weight : array-like of shape = [n_samples] or None, optional (default=None)\n",
      " |          Weights of training data.\n",
      " |      init_score : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task) or shape = [n_samples, n_classes] (for multi-class task) or None, optional (default=None)\n",
      " |          Init score of training data.\n",
      " |      eval_set : list or None, optional (default=None)\n",
      " |          A list of (X, y) tuple pairs to use as validation sets.\n",
      " |      eval_names : list of str, or None, optional (default=None)\n",
      " |          Names of eval_set.\n",
      " |      eval_sample_weight : list of array, or None, optional (default=None)\n",
      " |          Weights of eval data.\n",
      " |      eval_class_weight : list or None, optional (default=None)\n",
      " |          Class weights of eval data.\n",
      " |      eval_init_score : list of array, or None, optional (default=None)\n",
      " |          Init score of eval data.\n",
      " |      eval_metric : str, callable, list or None, optional (default=None)\n",
      " |          If str, it should be a built-in evaluation metric to use.\n",
      " |          If callable, it should be a custom evaluation metric, see note below for more details.\n",
      " |          If list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both.\n",
      " |          In either case, the ``metric`` from the model parameters will be evaluated and used as well.\n",
      " |          Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker.\n",
      " |      early_stopping_rounds : int or None, optional (default=None)\n",
      " |          Activates early stopping. The model will train until the validation score stops improving.\n",
      " |          Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
      " |          to continue training.\n",
      " |          Requires at least one validation data and one metric.\n",
      " |          If there's more than one, will check all of them. But the training data is ignored anyway.\n",
      " |          To check only the first metric, set the ``first_metric_only`` parameter to ``True``\n",
      " |          in additional parameters ``**kwargs`` of the model constructor.\n",
      " |      verbose : bool or int, optional (default=True)\n",
      " |          Requires at least one evaluation data.\n",
      " |          If True, the eval metric on the eval set is printed at each boosting stage.\n",
      " |          If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n",
      " |          The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n",
      " |      \n",
      " |          .. rubric:: Example\n",
      " |      \n",
      " |          With ``verbose`` = 4 and at least one item in ``eval_set``,\n",
      " |          an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
      " |      \n",
      " |      feature_name : list of str, or 'auto', optional (default='auto')\n",
      " |          Feature names.\n",
      " |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      " |      categorical_feature : list of str or int, or 'auto', optional (default='auto')\n",
      " |          Categorical features.\n",
      " |          If list of int, interpreted as indices.\n",
      " |          If list of str, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      " |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      " |          All values in categorical features should be less than int32 max value (2147483647).\n",
      " |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      " |          All negative values in categorical features will be treated as missing values.\n",
      " |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      " |      callbacks : list of callable, or None, optional (default=None)\n",
      " |          List of callback functions that are applied at each iteration.\n",
      " |          See Callbacks in Python API for more information.\n",
      " |      init_model : str, pathlib.Path, Booster, LGBMModel or None, optional (default=None)\n",
      " |          Filename of LightGBM model, Booster instance or LGBMModel instance used for continue training.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      Custom eval function expects a callable with following signatures:\n",
      " |      ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n",
      " |      ``func(y_true, y_pred, weight, group)``\n",
      " |      and returns (eval_name, eval_result, is_higher_better) or\n",
      " |      list of (eval_name, eval_result, is_higher_better):\n",
      " |      \n",
      " |          y_true : array-like of shape = [n_samples]\n",
      " |              The target values.\n",
      " |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The predicted values.\n",
      " |              In case of custom ``objective``, predicted values are returned before any transformation,\n",
      " |              e.g. they are raw margin instead of probability of positive class for binary task in this case.\n",
      " |          weight : array-like of shape = [n_samples]\n",
      " |              The weight of samples.\n",
      " |          group : array-like\n",
      " |              Group/query data.\n",
      " |              Only used in the learning-to-rank task.\n",
      " |              sum(group) = n_samples.\n",
      " |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      " |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      " |          eval_name : str\n",
      " |              The name of evaluation function (without whitespace).\n",
      " |          eval_result : float\n",
      " |              The eval result.\n",
      " |          is_higher_better : bool\n",
      " |              Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      " |      \n",
      " |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      " |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i].\n",
      " |  \n",
      " |  predict(self, X, raw_score=False, start_iteration=0, num_iteration=None, pred_leaf=False, pred_contrib=False, **kwargs)\n",
      " |      Return the predicted value for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      raw_score : bool, optional (default=False)\n",
      " |          Whether to predict raw scores.\n",
      " |      start_iteration : int, optional (default=0)\n",
      " |          Start index of the iteration to predict.\n",
      " |          If <= 0, starts from the first iteration.\n",
      " |      num_iteration : int or None, optional (default=None)\n",
      " |          Total number of iterations used in the prediction.\n",
      " |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      " |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      " |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      " |      pred_leaf : bool, optional (default=False)\n",
      " |          Whether to predict leaf index.\n",
      " |      pred_contrib : bool, optional (default=False)\n",
      " |          Whether to predict feature contributions.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      " |              like SHAP interaction values,\n",
      " |              you can install the shap package (https://github.com/slundberg/shap).\n",
      " |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      " |              column, where the last column is the expected value.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other parameters for the prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predicted_result : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      " |          The predicted values.\n",
      " |      X_leaves : array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      " |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      " |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\n",
      " |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      " |  \n",
      " |  predict_proba(self, X, raw_score=False, start_iteration=0, num_iteration=None, pred_leaf=False, pred_contrib=False, **kwargs)\n",
      " |      Return the predicted probability for each class for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      raw_score : bool, optional (default=False)\n",
      " |          Whether to predict raw scores.\n",
      " |      start_iteration : int, optional (default=0)\n",
      " |          Start index of the iteration to predict.\n",
      " |          If <= 0, starts from the first iteration.\n",
      " |      num_iteration : int or None, optional (default=None)\n",
      " |          Total number of iterations used in the prediction.\n",
      " |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      " |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      " |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      " |      pred_leaf : bool, optional (default=False)\n",
      " |          Whether to predict leaf index.\n",
      " |      pred_contrib : bool, optional (default=False)\n",
      " |          Whether to predict feature contributions.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      " |              like SHAP interaction values,\n",
      " |              you can install the shap package (https://github.com/slundberg/shap).\n",
      " |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      " |              column, where the last column is the expected value.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other parameters for the prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predicted_probability : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      " |          The predicted values.\n",
      " |      X_leaves : array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      " |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      " |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\n",
      " |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |      :obj:`array` of shape = [n_classes]: The class label array.\n",
      " |  \n",
      " |  n_classes_\n",
      " |      :obj:`int`: The number of classes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LGBMModel:\n",
      " |  \n",
      " |  __init__(self, boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[str, Callable, NoneType] = None, class_weight: Union[Dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: int = -1, silent: Union[bool, str] = 'warn', importance_type: str = 'split', **kwargs)\n",
      " |      Construct a gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      boosting_type : str, optional (default='gbdt')\n",
      " |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      " |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      " |          'goss', Gradient-based One-Side Sampling.\n",
      " |          'rf', Random Forest.\n",
      " |      num_leaves : int, optional (default=31)\n",
      " |          Maximum tree leaves for base learners.\n",
      " |      max_depth : int, optional (default=-1)\n",
      " |          Maximum tree depth for base learners, <=0 means no limit.\n",
      " |      learning_rate : float, optional (default=0.1)\n",
      " |          Boosting learning rate.\n",
      " |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      " |          in training using ``reset_parameter`` callback.\n",
      " |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      " |      n_estimators : int, optional (default=100)\n",
      " |          Number of boosted trees to fit.\n",
      " |      subsample_for_bin : int, optional (default=200000)\n",
      " |          Number of samples for constructing bins.\n",
      " |      objective : str, callable or None, optional (default=None)\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      " |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      " |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |          Use this parameter only for multi-class classification task;\n",
      " |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      " |          Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      " |          You may want to consider performing probability calibration\n",
      " |          (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      " |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      " |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |          If None, all classes are supposed to have weight one.\n",
      " |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      " |          if ``sample_weight`` is specified.\n",
      " |      min_split_gain : float, optional (default=0.)\n",
      " |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |      min_child_weight : float, optional (default=1e-3)\n",
      " |          Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
      " |      min_child_samples : int, optional (default=20)\n",
      " |          Minimum number of data needed in a child (leaf).\n",
      " |      subsample : float, optional (default=1.)\n",
      " |          Subsample ratio of the training instance.\n",
      " |      subsample_freq : int, optional (default=0)\n",
      " |          Frequency of subsample, <=0 means no enable.\n",
      " |      colsample_bytree : float, optional (default=1.)\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      reg_alpha : float, optional (default=0.)\n",
      " |          L1 regularization term on weights.\n",
      " |      reg_lambda : float, optional (default=0.)\n",
      " |          L2 regularization term on weights.\n",
      " |      random_state : int, RandomState object or None, optional (default=None)\n",
      " |          Random number seed.\n",
      " |          If int, this number is used to seed the C++ code.\n",
      " |          If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      " |          If None, default seeds in C++ code are used.\n",
      " |      n_jobs : int, optional (default=-1)\n",
      " |          Number of parallel threads.\n",
      " |      silent : bool, optional (default=True)\n",
      " |          Whether to print messages while running boosting.\n",
      " |      importance_type : str, optional (default='split')\n",
      " |          The type of feature importance to be filled into ``feature_importances_``.\n",
      " |          If 'split', result contains numbers of times the feature is used in a model.\n",
      " |          If 'gain', result contains total gains of splits which use the feature.\n",
      " |      **kwargs\n",
      " |          Other parameters for the model.\n",
      " |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      " |      \n",
      " |          .. warning::\n",
      " |      \n",
      " |              \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      A custom objective function can be provided for the ``objective`` parameter.\n",
      " |      In this case, it should have the signature\n",
      " |      ``objective(y_true, y_pred) -> grad, hess`` or\n",
      " |      ``objective(y_true, y_pred, group) -> grad, hess``:\n",
      " |      \n",
      " |          y_true : array-like of shape = [n_samples]\n",
      " |              The target values.\n",
      " |          y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The predicted values.\n",
      " |              Predicted values are returned before any transformation,\n",
      " |              e.g. they are raw margin instead of probability of positive class for binary task.\n",
      " |          group : array-like\n",
      " |              Group/query data.\n",
      " |              Only used in the learning-to-rank task.\n",
      " |              sum(group) = n_samples.\n",
      " |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      " |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      " |          grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The value of the first order derivative (gradient) of the loss\n",
      " |              with respect to the elements of y_pred for each sample point.\n",
      " |          hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
      " |              The value of the second order derivative (Hessian) of the loss\n",
      " |              with respect to the elements of y_pred for each sample point.\n",
      " |      \n",
      " |      For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
      " |      If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
      " |      and you should group grad and hess in this way as well.\n",
      " |  \n",
      " |  __sklearn_is_fitted__(self) -> bool\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, optional (default=True)\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params\n",
      " |          Parameter names with their new values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from LGBMModel:\n",
      " |  \n",
      " |  best_iteration_\n",
      " |      :obj:`int` or :obj:`None`: The best iteration of fitted model if ``early_stopping()`` callback has been specified.\n",
      " |  \n",
      " |  best_score_\n",
      " |      :obj:`dict`: The best score of fitted model.\n",
      " |  \n",
      " |  booster_\n",
      " |      Booster: The underlying Booster of this model.\n",
      " |  \n",
      " |  evals_result_\n",
      " |      :obj:`dict` or :obj:`None`: The evaluation results if validation sets have been specified.\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      :obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          ``importance_type`` attribute is passed to the function\n",
      " |          to configure the type of importance values to be extracted.\n",
      " |  \n",
      " |  feature_name_\n",
      " |      :obj:`array` of shape = [n_features]: The names of features.\n",
      " |  \n",
      " |  n_features_\n",
      " |      :obj:`int`: The number of features of fitted model.\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      :obj:`int`: The number of features of fitted model.\n",
      " |  \n",
      " |  objective_\n",
      " |      :obj:`str` or :obj:`callable`: The concrete objective used while fitting this model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d8eb77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907cd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb =  LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5a858c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b3f422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495827b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
