{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x830bICpQwj3"
      },
      "outputs": [],
      "source": [
        "# AlexNet\n",
        "# c-p-b c-p-b  c-c-c-p-b f d-do-d-do-d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iFJ59OdhRJ88"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 로드\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from tensorflow.keras.layers import Flatten,Conv2D,MaxPooling2D, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_-9FUJyjRTEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87af4c57-b9d4-42c5-e4df-56ee08199508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-04 06:42:46--  https://bit.ly/36QytdH\n",
            "Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11\n",
            "Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://onedrive.live.com/download?cid=822579D69D2DC3B5&resid=822579D69D2DC3B5!597859&authkey=AGd0CpvKFkK8GtE [following]\n",
            "--2023-01-04 06:42:46--  https://onedrive.live.com/download?cid=822579D69D2DC3B5&resid=822579D69D2DC3B5!597859&authkey=AGd0CpvKFkK8GtE\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://57ucia.bl.files.1drv.com/y4m49JMfM82ufE3aGLR_nZ7s3yVmlpN6QB_S3xUxf5pKaLXyMPbl5lzthl0n8XZ9L1hdPG-EyQcBO1t0nFZo35gNeBskAzCp7WUtuF4aNy-RShmkcMup-mcgfsr3-euEXCVYU1ro4jYJEkk3I-FrjXzfsc_jfak_w9oZaztV0qYB2MeoHScjsoX__aUe9eC5r57ikPKJC-O2lq_ZNEsTlSbhw/oxflower17.npz?download&psid=1 [following]\n",
            "--2023-01-04 06:42:47--  https://57ucia.bl.files.1drv.com/y4m49JMfM82ufE3aGLR_nZ7s3yVmlpN6QB_S3xUxf5pKaLXyMPbl5lzthl0n8XZ9L1hdPG-EyQcBO1t0nFZo35gNeBskAzCp7WUtuF4aNy-RShmkcMup-mcgfsr3-euEXCVYU1ro4jYJEkk3I-FrjXzfsc_jfak_w9oZaztV0qYB2MeoHScjsoX__aUe9eC5r57ikPKJC-O2lq_ZNEsTlSbhw/oxflower17.npz?download&psid=1\n",
            "Resolving 57ucia.bl.files.1drv.com (57ucia.bl.files.1drv.com)... 13.107.42.12\n",
            "Connecting to 57ucia.bl.files.1drv.com (57ucia.bl.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252415092 (241M) [application/zip]\n",
            "Saving to: ‘oxflower17.npz’\n",
            "\n",
            "oxflower17.npz      100%[===================>] 240.72M  16.6MB/s    in 18s     \n",
            "\n",
            "2023-01-04 06:43:07 (13.3 MB/s) - ‘oxflower17.npz’ saved [252415092/252415092]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://bit.ly/36QytdH -O oxflower17.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BteYl3YZRajR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "data = np.load('/content/oxflower17.npz')\n",
        "X = data['X']\n",
        "Y = data['Y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hSuk5-n-SxGh"
      },
      "outputs": [],
      "source": [
        "# 로컬용\n",
        "# import urllib\n",
        "# import numpy as np\n",
        "\n",
        "# urllib.request.urlretrieve('https://bit.ly/36QytdH','a.npz')\n",
        "# data = np.load('./a.npz')\n",
        "# X = data['X']\n",
        "# Y = data['Y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i93kaDVS-gC",
        "outputId": "a7c90e2f-c25b-452f-d3fc-70700b6a84a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1360, 224, 224, 3), (1360, 17))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X.shape, Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YPGR2QyrU4th"
      },
      "outputs": [],
      "source": [
        "# 신경망모델\n",
        "# AlexNet\n",
        "# c-p-b c-p-b  c-c-c-p-b f d-do-d-do-d\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(96, (11,11), strides = (4,4),activation='relu', input_shape=(224,224,3) )  )\n",
        "model.add(MaxPooling2D(pool_size=(3,3),strides = (2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(256, (5,5),activation='relu' )  )\n",
        "model.add(MaxPooling2D(pool_size=(3,3),strides = (2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(256, (3,3),activation='relu' )  )\n",
        "model.add(Conv2D(384, (3,3),activation='relu' )  )\n",
        "model.add(Conv2D(384, (3,3),activation='relu' )  )\n",
        "model.add(MaxPooling2D(pool_size=(3,3),strides = (2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4096, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(17, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Vuhp6ailVpeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74142b3-8252-4224-cf13-8d8dc727593a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 26, 26, 96)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 26, 26, 96)       384       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 22, 22, 256)       614656    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 10, 10, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 10, 10, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 6, 6, 384)         885120    \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 4, 4, 384)         1327488   \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 1, 384)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1, 1, 384)        1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 384)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              1576960   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 17)                69649     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,883,153\n",
            "Trainable params: 21,881,681\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDk07PlPgJ5s",
        "outputId": "1b26451c-6d1e-4bdf-a14c-0c43f73d3326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - 2s 86ms/step - loss: 4.2499 - accuracy: 0.2418 - val_loss: 7.2807 - val_accuracy: 0.0294\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 1s 70ms/step - loss: 3.3439 - accuracy: 0.2680 - val_loss: 5.1633 - val_accuracy: 0.0809\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 1s 68ms/step - loss: 2.4035 - accuracy: 0.3652 - val_loss: 3.3311 - val_accuracy: 0.1544\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 2.3300 - accuracy: 0.3889 - val_loss: 4.1896 - val_accuracy: 0.1765\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 1s 59ms/step - loss: 2.3982 - accuracy: 0.3881 - val_loss: 4.3013 - val_accuracy: 0.0735\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 2.5501 - accuracy: 0.3725 - val_loss: 4.8682 - val_accuracy: 0.1691\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 2.4196 - accuracy: 0.3840 - val_loss: 4.1007 - val_accuracy: 0.2279\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 2.1800 - accuracy: 0.4363 - val_loss: 3.8089 - val_accuracy: 0.3235\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 2.3154 - accuracy: 0.4003 - val_loss: 3.4081 - val_accuracy: 0.2574\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 2.1336 - accuracy: 0.4175 - val_loss: 3.7388 - val_accuracy: 0.2206\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 1.9374 - accuracy: 0.4624 - val_loss: 3.0483 - val_accuracy: 0.2647\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 1s 59ms/step - loss: 1.8932 - accuracy: 0.4926 - val_loss: 2.8301 - val_accuracy: 0.4118\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 1.9616 - accuracy: 0.4992 - val_loss: 4.0896 - val_accuracy: 0.3088\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 1.8010 - accuracy: 0.5376 - val_loss: 3.4604 - val_accuracy: 0.3088\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 1.7573 - accuracy: 0.5212 - val_loss: 2.1615 - val_accuracy: 0.4926\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 1s 65ms/step - loss: 1.5441 - accuracy: 0.5596 - val_loss: 3.3072 - val_accuracy: 0.3456\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 1.7711 - accuracy: 0.5433 - val_loss: 2.4852 - val_accuracy: 0.3897\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 2.0798 - accuracy: 0.4796 - val_loss: 4.2098 - val_accuracy: 0.3456\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 1.5826 - accuracy: 0.5441 - val_loss: 3.4571 - val_accuracy: 0.3603\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 1.5888 - accuracy: 0.5490 - val_loss: 2.6582 - val_accuracy: 0.4118\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 1.5574 - accuracy: 0.5645 - val_loss: 2.6457 - val_accuracy: 0.4265\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 1s 64ms/step - loss: 1.4073 - accuracy: 0.6021 - val_loss: 2.8135 - val_accuracy: 0.4338\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 1s 64ms/step - loss: 1.4907 - accuracy: 0.5956 - val_loss: 2.4543 - val_accuracy: 0.4779\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 1.4865 - accuracy: 0.5882 - val_loss: 2.1073 - val_accuracy: 0.4926\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 1.3507 - accuracy: 0.6209 - val_loss: 3.4709 - val_accuracy: 0.4412\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 1.4677 - accuracy: 0.6413 - val_loss: 3.5431 - val_accuracy: 0.4706\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 1.1532 - accuracy: 0.6618 - val_loss: 3.3618 - val_accuracy: 0.5294\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.9918 - accuracy: 0.7018 - val_loss: 2.8081 - val_accuracy: 0.4853\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 1s 64ms/step - loss: 1.0359 - accuracy: 0.7141 - val_loss: 2.9580 - val_accuracy: 0.5294\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 1.1292 - accuracy: 0.6912 - val_loss: 2.7338 - val_accuracy: 0.5515\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 1s 68ms/step - loss: 1.1849 - accuracy: 0.6846 - val_loss: 3.7507 - val_accuracy: 0.3897\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 1s 68ms/step - loss: 1.0140 - accuracy: 0.7141 - val_loss: 2.7608 - val_accuracy: 0.4853\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 1s 64ms/step - loss: 0.8647 - accuracy: 0.7369 - val_loss: 3.1099 - val_accuracy: 0.5368\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.9167 - accuracy: 0.7484 - val_loss: 4.0750 - val_accuracy: 0.3750\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.8852 - accuracy: 0.7516 - val_loss: 2.5578 - val_accuracy: 0.5294\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.8713 - accuracy: 0.7402 - val_loss: 4.1972 - val_accuracy: 0.3897\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.6269 - accuracy: 0.7941 - val_loss: 3.1710 - val_accuracy: 0.5147\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 1s 59ms/step - loss: 0.8951 - accuracy: 0.7565 - val_loss: 3.2033 - val_accuracy: 0.5074\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.8656 - accuracy: 0.7533 - val_loss: 2.9744 - val_accuracy: 0.5515\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.8071 - accuracy: 0.7729 - val_loss: 2.7333 - val_accuracy: 0.5956\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.7362 - accuracy: 0.7998 - val_loss: 2.5875 - val_accuracy: 0.5662\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.7726 - accuracy: 0.7917 - val_loss: 3.1953 - val_accuracy: 0.5147\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.5997 - accuracy: 0.8121 - val_loss: 3.4056 - val_accuracy: 0.4853\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.4454 - accuracy: 0.8562 - val_loss: 2.3540 - val_accuracy: 0.6397\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 1s 63ms/step - loss: 0.4802 - accuracy: 0.8546 - val_loss: 3.4052 - val_accuracy: 0.5294\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.5203 - accuracy: 0.8529 - val_loss: 3.0314 - val_accuracy: 0.4926\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 1.0696 - accuracy: 0.7279 - val_loss: 2.7972 - val_accuracy: 0.5441\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.7104 - accuracy: 0.8121 - val_loss: 2.7917 - val_accuracy: 0.5809\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.5023 - accuracy: 0.8513 - val_loss: 3.1199 - val_accuracy: 0.5221\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.3362 - accuracy: 0.8987 - val_loss: 3.9684 - val_accuracy: 0.5368\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.7842 - accuracy: 0.7933 - val_loss: 2.7468 - val_accuracy: 0.6103\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.6520 - accuracy: 0.8194 - val_loss: 3.7921 - val_accuracy: 0.5147\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.7266 - accuracy: 0.8186 - val_loss: 3.4615 - val_accuracy: 0.5074\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.5227 - accuracy: 0.8619 - val_loss: 4.1163 - val_accuracy: 0.4853\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.2603 - accuracy: 0.9118 - val_loss: 3.1337 - val_accuracy: 0.5956\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.4065 - accuracy: 0.8775 - val_loss: 3.0216 - val_accuracy: 0.5588\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.6048 - accuracy: 0.8472 - val_loss: 3.0158 - val_accuracy: 0.6103\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.3559 - accuracy: 0.8848 - val_loss: 3.3273 - val_accuracy: 0.5956\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.4798 - accuracy: 0.8766 - val_loss: 2.7990 - val_accuracy: 0.5882\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.4722 - accuracy: 0.8701 - val_loss: 3.0716 - val_accuracy: 0.5588\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.5828 - accuracy: 0.8489 - val_loss: 3.7353 - val_accuracy: 0.5221\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.5732 - accuracy: 0.8505 - val_loss: 3.1663 - val_accuracy: 0.5441\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.3634 - accuracy: 0.8922 - val_loss: 3.2662 - val_accuracy: 0.5368\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.3098 - accuracy: 0.9085 - val_loss: 4.0343 - val_accuracy: 0.4632\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.2649 - accuracy: 0.9216 - val_loss: 5.6468 - val_accuracy: 0.5294\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.4018 - accuracy: 0.8815 - val_loss: 3.2579 - val_accuracy: 0.6103\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.2922 - accuracy: 0.9191 - val_loss: 4.4815 - val_accuracy: 0.5515\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.2831 - accuracy: 0.9118 - val_loss: 3.2714 - val_accuracy: 0.5515\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.3187 - accuracy: 0.9069 - val_loss: 3.2682 - val_accuracy: 0.5662\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.1665 - accuracy: 0.9436 - val_loss: 2.5775 - val_accuracy: 0.7206\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.2726 - accuracy: 0.9175 - val_loss: 7.9598 - val_accuracy: 0.4485\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.2363 - accuracy: 0.9306 - val_loss: 4.1883 - val_accuracy: 0.5515\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.4712 - accuracy: 0.9020 - val_loss: 4.7009 - val_accuracy: 0.5221\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.4123 - accuracy: 0.8856 - val_loss: 4.4153 - val_accuracy: 0.5368\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.4244 - accuracy: 0.8922 - val_loss: 4.3171 - val_accuracy: 0.5074\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.2506 - accuracy: 0.9199 - val_loss: 2.7873 - val_accuracy: 0.6397\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.1804 - accuracy: 0.9453 - val_loss: 4.1590 - val_accuracy: 0.5662\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.0837 - accuracy: 0.9706 - val_loss: 4.2836 - val_accuracy: 0.6029\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.1852 - accuracy: 0.9510 - val_loss: 3.6592 - val_accuracy: 0.6029\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.2853 - accuracy: 0.9322 - val_loss: 3.1957 - val_accuracy: 0.6691\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.2001 - accuracy: 0.9428 - val_loss: 3.4579 - val_accuracy: 0.6471\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 1s 64ms/step - loss: 0.1902 - accuracy: 0.9469 - val_loss: 3.1622 - val_accuracy: 0.6765\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 1s 62ms/step - loss: 0.0943 - accuracy: 0.9673 - val_loss: 3.7737 - val_accuracy: 0.6544\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 1s 62ms/step - loss: 0.1215 - accuracy: 0.9632 - val_loss: 3.6706 - val_accuracy: 0.6029\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.4902 - accuracy: 0.8979 - val_loss: 2.4815 - val_accuracy: 0.6544\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 1s 62ms/step - loss: 0.1971 - accuracy: 0.9526 - val_loss: 5.0491 - val_accuracy: 0.5588\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.1041 - accuracy: 0.9714 - val_loss: 3.0997 - val_accuracy: 0.6691\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.3036 - accuracy: 0.9191 - val_loss: 4.0650 - val_accuracy: 0.5956\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.2970 - accuracy: 0.9199 - val_loss: 3.4579 - val_accuracy: 0.6103\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.2916 - accuracy: 0.9224 - val_loss: 3.5483 - val_accuracy: 0.5809\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.4125 - accuracy: 0.9142 - val_loss: 5.1101 - val_accuracy: 0.5147\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 1s 62ms/step - loss: 0.4927 - accuracy: 0.8962 - val_loss: 3.2574 - val_accuracy: 0.6397\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.4076 - accuracy: 0.8995 - val_loss: 4.8042 - val_accuracy: 0.5662\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 1s 62ms/step - loss: 0.7184 - accuracy: 0.8562 - val_loss: 3.7971 - val_accuracy: 0.6324\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 1s 65ms/step - loss: 0.3208 - accuracy: 0.9134 - val_loss: 3.2855 - val_accuracy: 0.6324\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.1520 - accuracy: 0.9559 - val_loss: 3.9531 - val_accuracy: 0.6250\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.1113 - accuracy: 0.9665 - val_loss: 3.3006 - val_accuracy: 0.6691\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.1503 - accuracy: 0.9592 - val_loss: 3.5451 - val_accuracy: 0.6765\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 1s 62ms/step - loss: 0.1149 - accuracy: 0.9698 - val_loss: 3.3495 - val_accuracy: 0.6765\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.2495 - accuracy: 0.9502 - val_loss: 3.5874 - val_accuracy: 0.6618\n"
          ]
        }
      ],
      "source": [
        "# 훈련\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(X,Y, batch_size=64, epochs=100, verbose=1, validation_split=0.1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WGDrjR0EgnXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df47e563-913e-4be5-d75d-910097ca46ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method fit in module keras.engine.training:\n",
            "\n",
            "fit(x=None, y=None, batch_size=None, epochs=1, verbose='auto', callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) method of keras.engine.sequential.Sequential instance\n",
            "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            "    \n",
            "    Args:\n",
            "        x: Input data. It could be:\n",
            "          - A Numpy array (or array-like), or a list of arrays\n",
            "            (in case the model has multiple inputs).\n",
            "          - A TensorFlow tensor, or a list of tensors\n",
            "            (in case the model has multiple inputs).\n",
            "          - A dict mapping input names to the corresponding array/tensors,\n",
            "            if the model has named inputs.\n",
            "          - A `tf.data` dataset. Should return a tuple\n",
            "            of either `(inputs, targets)` or\n",
            "            `(inputs, targets, sample_weights)`.\n",
            "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            "            or `(inputs, targets, sample_weights)`.\n",
            "          - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
            "            callable that takes a single argument of type\n",
            "            `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
            "            `DatasetCreator` should be used when users prefer to specify the\n",
            "            per-replica batching and sharding logic for the `Dataset`.\n",
            "            See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
            "            information.\n",
            "          A more detailed description of unpacking behavior for iterator types\n",
            "          (Dataset, generator, Sequence) is given below. If using\n",
            "          `tf.distribute.experimental.ParameterServerStrategy`, only\n",
            "          `DatasetCreator` type is supported for `x`.\n",
            "        y: Target data. Like the input data `x`,\n",
            "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
            "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
            "          or `keras.utils.Sequence` instance, `y` should\n",
            "          not be specified (since targets will be obtained from `x`).\n",
            "        batch_size: Integer or `None`.\n",
            "            Number of samples per gradient update.\n",
            "            If unspecified, `batch_size` will default to 32.\n",
            "            Do not specify the `batch_size` if your data is in the\n",
            "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
            "            (since they generate batches).\n",
            "        epochs: Integer. Number of epochs to train the model.\n",
            "            An epoch is an iteration over the entire `x` and `y`\n",
            "            data provided\n",
            "            (unless the `steps_per_epoch` flag is set to\n",
            "            something other than None).\n",
            "            Note that in conjunction with `initial_epoch`,\n",
            "            `epochs` is to be understood as \"final epoch\".\n",
            "            The model is not trained for a number of iterations\n",
            "            given by `epochs`, but merely until the epoch\n",
            "            of index `epochs` is reached.\n",
            "        verbose: 'auto', 0, 1, or 2. Verbosity mode.\n",
            "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            "            'auto' defaults to 1 for most cases, but 2 when used with\n",
            "            `ParameterServerStrategy`. Note that the progress bar is not\n",
            "            particularly useful when logged to a file, so verbose=2 is\n",
            "            recommended when not running interactively (eg, in a production\n",
            "            environment).\n",
            "        callbacks: List of `keras.callbacks.Callback` instances.\n",
            "            List of callbacks to apply during training.\n",
            "            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            "            and `tf.keras.callbacks.History` callbacks are created automatically\n",
            "            and need not be passed into `model.fit`.\n",
            "            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            "            `verbose` argument to `model.fit`.\n",
            "            Callbacks with batch-level calls are currently unsupported with\n",
            "            `tf.distribute.experimental.ParameterServerStrategy`, and users are\n",
            "            advised to implement epoch-level calls instead with an appropriate\n",
            "            `steps_per_epoch` value.\n",
            "        validation_split: Float between 0 and 1.\n",
            "            Fraction of the training data to be used as validation data.\n",
            "            The model will set apart this fraction of the training data,\n",
            "            will not train on it, and will evaluate\n",
            "            the loss and any model metrics\n",
            "            on this data at the end of each epoch.\n",
            "            The validation data is selected from the last samples\n",
            "            in the `x` and `y` data provided, before shuffling. This argument is\n",
            "            not supported when `x` is a dataset, generator or\n",
            "            `keras.utils.Sequence` instance.\n",
            "            If both `validation_data` and `validation_split` are provided,\n",
            "            `validation_data` will override `validation_split`.\n",
            "            `validation_split` is not yet supported with\n",
            "            `tf.distribute.experimental.ParameterServerStrategy`.\n",
            "        validation_data: Data on which to evaluate\n",
            "            the loss and any model metrics at the end of each epoch.\n",
            "            The model will not be trained on this data. Thus, note the fact\n",
            "            that the validation loss of data provided using `validation_split`\n",
            "            or `validation_data` is not affected by regularization layers like\n",
            "            noise and dropout.\n",
            "            `validation_data` will override `validation_split`.\n",
            "            `validation_data` could be:\n",
            "              - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n",
            "              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n",
            "              - A `tf.data.Dataset`.\n",
            "              - A Python generator or `keras.utils.Sequence` returning\n",
            "              `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
            "            `validation_data` is not yet supported with\n",
            "            `tf.distribute.experimental.ParameterServerStrategy`.\n",
            "        shuffle: Boolean (whether to shuffle the training data\n",
            "            before each epoch) or str (for 'batch'). This argument is ignored\n",
            "            when `x` is a generator or an object of tf.data.Dataset.\n",
            "            'batch' is a special option for dealing\n",
            "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
            "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            "        class_weight: Optional dictionary mapping class indices (integers)\n",
            "            to a weight (float) value, used for weighting the loss function\n",
            "            (during training only).\n",
            "            This can be useful to tell the model to\n",
            "            \"pay more attention\" to samples from\n",
            "            an under-represented class.\n",
            "        sample_weight: Optional Numpy array of weights for\n",
            "            the training samples, used for weighting the loss function\n",
            "            (during training only). You can either pass a flat (1D)\n",
            "            Numpy array with the same length as the input samples\n",
            "            (1:1 mapping between weights and samples),\n",
            "            or in the case of temporal data,\n",
            "            you can pass a 2D array with shape\n",
            "            `(samples, sequence_length)`,\n",
            "            to apply a different weight to every timestep of every sample. This\n",
            "            argument is not supported when `x` is a dataset, generator, or\n",
            "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            "            as the third element of `x`.\n",
            "        initial_epoch: Integer.\n",
            "            Epoch at which to start training\n",
            "            (useful for resuming a previous training run).\n",
            "        steps_per_epoch: Integer or `None`.\n",
            "            Total number of steps (batches of samples)\n",
            "            before declaring one epoch finished and starting the\n",
            "            next epoch. When training with input tensors such as\n",
            "            TensorFlow data tensors, the default `None` is equal to\n",
            "            the number of samples in your dataset divided by\n",
            "            the batch size, or 1 if that cannot be determined. If x is a\n",
            "            `tf.data` dataset, and 'steps_per_epoch'\n",
            "            is None, the epoch will run until the input dataset is exhausted.\n",
            "            When passing an infinitely repeating dataset, you must specify the\n",
            "            `steps_per_epoch` argument. If `steps_per_epoch=-1` the training\n",
            "            will run indefinitely with an infinitely repeating dataset.\n",
            "            This argument is not supported with array inputs.\n",
            "            When using `tf.distribute.experimental.ParameterServerStrategy`:\n",
            "              * `steps_per_epoch=None` is not supported.\n",
            "        validation_steps: Only relevant if `validation_data` is provided and\n",
            "            is a `tf.data` dataset. Total number of steps (batches of\n",
            "            samples) to draw before stopping when performing validation\n",
            "            at the end of every epoch. If 'validation_steps' is None, validation\n",
            "            will run until the `validation_data` dataset is exhausted. In the\n",
            "            case of an infinitely repeated dataset, it will run into an\n",
            "            infinite loop. If 'validation_steps' is specified and only part of\n",
            "            the dataset will be consumed, the evaluation will start from the\n",
            "            beginning of the dataset at each epoch. This ensures that the same\n",
            "            validation samples are used every time.\n",
            "        validation_batch_size: Integer or `None`.\n",
            "            Number of samples per validation batch.\n",
            "            If unspecified, will default to `batch_size`.\n",
            "            Do not specify the `validation_batch_size` if your data is in the\n",
            "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
            "            (since they generate batches).\n",
            "        validation_freq: Only relevant if validation data is provided. Integer\n",
            "            or `collections.abc.Container` instance (e.g. list, tuple, etc.).\n",
            "            If an integer, specifies how many training epochs to run before a\n",
            "            new validation run is performed, e.g. `validation_freq=2` runs\n",
            "            validation every 2 epochs. If a Container, specifies the epochs on\n",
            "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            "            input only. Maximum size for the generator queue.\n",
            "            If unspecified, `max_queue_size` will default to 10.\n",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            "            only. Maximum number of processes to spin up\n",
            "            when using process-based threading. If unspecified, `workers`\n",
            "            will default to 1.\n",
            "        use_multiprocessing: Boolean. Used for generator or\n",
            "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            "            threading. If unspecified, `use_multiprocessing` will default to\n",
            "            `False`. Note that because this implementation relies on\n",
            "            multiprocessing, you should not pass non-picklable arguments to\n",
            "            the generator as they can't be passed easily to children processes.\n",
            "    \n",
            "    Unpacking behavior for iterator-like inputs:\n",
            "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
            "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            "      yield not only features (x) but optionally targets (y) and sample weights.\n",
            "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
            "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            "      second and third elements will be used for y and sample_weight\n",
            "      respectively. Any other type provided will be wrapped in a length one\n",
            "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            "      should still adhere to the top-level tuple structure.\n",
            "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            "      features, targets, and weights from the keys of a single dict.\n",
            "        A notable unsupported data type is the namedtuple. The reason is that\n",
            "      it behaves like both an ordered datatype (tuple) and a mapping\n",
            "      datatype (dict). So given a namedtuple of the form:\n",
            "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            "      it is ambiguous whether to reverse the order of the elements when\n",
            "      interpreting the value. Even worse is a tuple of the form:\n",
            "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            "      and sample_weight or passed through as a single element to `x`. As a\n",
            "      result the data processing code will simply raise a ValueError if it\n",
            "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            "    \n",
            "    Returns:\n",
            "        A `History` object. Its `History.history` attribute is\n",
            "        a record of training loss values and metrics values\n",
            "        at successive epochs, as well as validation loss values\n",
            "        and validation metrics values (if applicable).\n",
            "    \n",
            "    Raises:\n",
            "        RuntimeError: 1. If the model was never compiled or,\n",
            "        2. If `model.fit` is  wrapped in `tf.function`.\n",
            "    \n",
            "        ValueError: In case of mismatch between the provided input data\n",
            "            and what the model expects or when the input data is empty.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(model.fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f9nGaLvAgoKx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}